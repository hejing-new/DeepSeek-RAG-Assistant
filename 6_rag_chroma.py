import streamlit as st
from openai import OpenAI
import PyPDF2
import chromadb # <--- æ–°æœ‹å‹ï¼šå‘é‡æ•°æ®åº“

# --- 1. é¡µé¢è®¾ç½® ---
st.set_page_config(page_title="RAG ä¸“ä¸šç‰ˆ (ChromaDB)", layout="wide")
st.title("ðŸ§  ä¹Ÿå°±æ˜¯æ‰€è°“çš„â€œç¬¬äºŒå¤§è„‘â€ (ChromaDB ç‰ˆ)")
st.caption("çŽ°åœ¨ï¼Œä½ å¯ä»¥ä¸Šä¼ å‡ ç™¾é¡µçš„ä¹¦ï¼Œæˆ‘ä¹Ÿèƒ½çž¬é—´æ‰¾åˆ°ç­”æ¡ˆï¼")

# --- 2. åˆå§‹åŒ– ChromaDB (æŒä¹…åŒ–å­˜å‚¨) ---
# è¿™è¡Œä»£ç ä¼šåœ¨ä½ å½“å‰ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªå« "my_knowledge_base" çš„æ–‡ä»¶å¤¹ç”¨æ¥å­˜æ•°æ®
# å“ªæ€•ä½ å…³æŽ‰ç½‘é¡µï¼Œæ•°æ®ä¾ç„¶åœ¨ï¼
chroma_client = chromadb.PersistentClient(path="./my_knowledge_base")

# åˆ›å»ºæˆ–èŽ·å–ä¸€ä¸ªâ€œé›†åˆâ€ï¼ˆç±»ä¼¼äºŽæ•°æ®åº“é‡Œçš„è¡¨ï¼‰
collection = chroma_client.get_or_create_collection(name="my_documents")

# --- 3. åˆå§‹åŒ– DeepSeek ---
with st.sidebar:
    api_key = st.text_input("è¯·è¾“å…¥ DeepSeek API Key", type="password", value="sk-è¿™é‡Œå¡«ä½ çš„Key")

client = OpenAI(
    api_key=input("è¯·è¾“å…¥Key:"),
    base_url="https://api.deepseek.com"
)

# åˆå§‹åŒ–èŠå¤©è®°å½•
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- 4. è¾…åŠ©å‡½æ•°ï¼šæ–‡æœ¬åˆ‡ç‰‡ (Chunking) ---
# å·¥ä¸šçº§åº”ç”¨é‡Œé€šå¸¸ç”¨ LangChain åˆ‡ï¼Œè¿™é‡Œæˆ‘ä»¬æ‰‹å†™ä¸€ä¸ªç®€å•çš„
# æŠŠé•¿æ–‡ç« åˆ‡æˆæ¯å— 300 å­—çš„å°æ®µ
def split_text(text, chunk_size=300):
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunks.append(text[i:i+chunk_size])
    return chunks

# --- 5. ä¾§è¾¹æ ï¼šä¸Šä¼ å¹¶å¤„ç†æ–‡ä»¶ ---
with st.sidebar:
    st.header("ðŸ“‚ çŸ¥è¯†åº“ç®¡ç†")
    uploaded_file = st.file_uploader("ä¸Šä¼  PDF æŠ•å–‚ç»™æ•°æ®åº“", type=["pdf"])
    
    if uploaded_file:
        # æŒ‰é’®ï¼šé˜²æ­¢æ¯æ¬¡åˆ·æ–°éƒ½é‡æ–°è¯»æ–‡ä»¶
        if st.button("å¼€å§‹å¤„ç†å¹¶å­˜å…¥æ•°æ®åº“"):
            with st.spinner("æ­£åœ¨åˆ‡ç‰‡å¹¶å­˜å…¥ Chroma... (ç¬¬ä¸€æ¬¡è¿è¡Œéœ€ä¸‹è½½æ¨¡åž‹ï¼Œè¯·ç¨å€™)"):
                try:
                    # A. è¯»å– PDF
                    pdf_reader = PyPDF2.PdfReader(uploaded_file)
                    full_text = ""
                    for page in pdf_reader.pages:
                        full_text += page.extract_text()
                    
                    # B. åˆ‡ç‰‡ (Chunking)
                    chunks = split_text(full_text)
                    st.write(f"ðŸ“Š æ–‡ç« å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")

                    # C. å­˜å…¥ Chroma
                    # ids å¿…é¡»æ˜¯å”¯ä¸€çš„ï¼Œæˆ‘ä»¬ç®€å•ç”¨æ–‡ä»¶å+åºå·
                    ids = [f"{uploaded_file.name}_{i}" for i in range(len(chunks))]
                    
                    # è¿™ä¸€æ­¥æœ€å…³é”®ï¼Chroma ä¼šè‡ªåŠ¨æŠŠæ–‡å­—å˜æˆå‘é‡å­˜èµ·æ¥
                    collection.add(
                        documents=chunks,
                        ids=ids
                    )
                    st.success("âœ… æˆåŠŸå­˜å…¥å‘é‡æ•°æ®åº“ï¼")
                except Exception as e:
                    st.error(f"å‡ºé”™å•¦ï¼š{e}")

    # æ˜¾ç¤ºæ•°æ®åº“å½“å‰çŠ¶æ€
    count = collection.count()
    st.info(f"ðŸ“š å½“å‰çŸ¥è¯†åº“é‡Œå…±æœ‰ {count} ä¸ªçŸ¥è¯†ç‰‡æ®µ")
    
    if st.button("æ¸…ç©ºçŸ¥è¯†åº“"):
        chroma_client.delete_collection("my_documents")
        st.experimental_rerun()

# --- 6. èŠå¤©ç•Œé¢ ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- 7. æ ¸å¿ƒé€»è¾‘ï¼šæ£€ç´¢ (Retrieval) + ç”Ÿæˆ (Generation) ---
if user_input := st.chat_input("è¯·é—®çŸ¥è¯†åº“..."):
    
    with st.chat_message("user"):
        st.markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    # A. æ£€ç´¢ (Retrieval)
    # åŽ» Chroma é‡Œæœï¼Œæ‰¾å‡ºè·Ÿé—®é¢˜æœ€ç›¸å…³çš„ 3 ä¸ªç‰‡æ®µ (n_results=3)
    results = collection.query(
        query_texts=[user_input],
        n_results=3
    )
    
    # æŠŠæœåˆ°çš„ 3 æ®µæ–‡å­—æ‹¼èµ·æ¥
    retrieved_text = "\n\n".join(results['documents'][0])
    
    # è°ƒè¯•ä¿¡æ¯ï¼šè®©ä½ çœ‹çœ‹ AI åˆ°åº•å‚è€ƒäº†å“ªäº›å†…å®¹ï¼ˆå¼€å‘æ—¶å¾ˆæœ‰ç”¨ï¼‰
    with st.expander("ðŸ•µï¸â€â™‚ï¸ æˆ‘å‚è€ƒäº†ä»¥ä¸‹ç‰‡æ®µ (RAG Debug)"):
        st.text(retrieved_text)

    # B. ç”Ÿæˆ (Generation)
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªåŸºäºŽçŸ¥è¯†åº“çš„æ™ºèƒ½åŠ©æ‰‹ã€‚
    è¯·ä¸¥æ ¼æ ¹æ®ä¸‹é¢çš„ã€å‚è€ƒèµ„æ–™ã€‘å›žç­”ç”¨æˆ·é—®é¢˜ã€‚
    å¦‚æžœèµ„æ–™é‡Œæ²¡æœ‰æåˆ°ï¼Œå°±è¯´ä¸çŸ¥é“ã€‚

    ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
    {retrieved_text}
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": system_prompt},
                *st.session_state.messages
            ]
        )
        ai_reply = response.choices[0].message.content

        with st.chat_message("assistant"):
            st.markdown(ai_reply)
        st.session_state.messages.append({"role": "assistant", "content": ai_reply})
        
    except Exception as e:
        st.error(f"API å‡ºé”™ï¼š{e}")